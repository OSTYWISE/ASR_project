{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch-audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "T=50\n",
    "C=20\n",
    "target_lengths = torch.randint(low=1, high=50, size=(), dtype=torch.long)\n",
    "target = torch.randint(low=1, high=20, size=(target_lengths,), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10), tensor([ 6,  9,  5,  5,  6, 15,  7,  3,  6,  1]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lengths, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ostywise/Downloads/ML/DLA/week04/.venv/lib/python3.12/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "/Users/ostywise/Downloads/ML/DLA/week04/.venv/lib/python3.12/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
      "  >>> augment = Gain(..., output_type='dict')\n",
      "  >>> augmented_samples = augment(samples).samples\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.text_encoder import CTCTextEncoder\n",
    "from src.datasets import LibrispeechDataset\n",
    "from src.transforms.wav_augs import Gain\n",
    "from src.datasets.collate import collate_fn\n",
    "import torch\n",
    "import torchaudio \n",
    "\n",
    "part=\"dev-clean\"\n",
    "instance_transforms= {\n",
    "    'get_spectrogram': torchaudio.transforms.MelSpectrogram(sample_rate=16000),\n",
    "    'audio': Gain()\n",
    "}\n",
    "\n",
    "text_encoder = CTCTextEncoder()\n",
    "dataset = LibrispeechDataset(\n",
    "    text_encoder=text_encoder,\n",
    "    part= part,\n",
    "    max_audio_length= 20.0,\n",
    "    max_text_length= 200,\n",
    "    limit= 10,\n",
    "    instance_transforms= instance_transforms\n",
    ")\n",
    "# dataloaders, batch_transforms = get_dataloaders(config, text_encoder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    batch_size=10,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    dataset=dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['audio', 'spectrogram', 'text_encoded', 'text', 'audio_path', 'spectrogram_length', 'text_encoded_length'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[176, 83, 161, 37, 65, 52, 136, 36, 147, 51]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['text_encoded_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CTCLoss\n",
    "criterion = CTCLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCTCLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mblank\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mzero_infinity\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "The Connectionist Temporal Classification loss.\n",
      "\n",
      "Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the\n",
      "probability of possible alignments of input to target, producing a loss value which is differentiable\n",
      "with respect to each input node. The alignment of input to target is assumed to be \"many-to-one\", which\n",
      "limits the length of the target sequence such that it must be :math:`\\leq` the input length.\n",
      "\n",
      "Args:\n",
      "    blank (int, optional): blank label. Default :math:`0`.\n",
      "    reduction (str, optional): Specifies the reduction to apply to the output:\n",
      "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "        ``'mean'``: the output losses will be divided by the target lengths and\n",
      "        then the mean over the batch is taken, ``'sum'``: the output losses will be summed.\n",
      "        Default: ``'mean'``\n",
      "    zero_infinity (bool, optional):\n",
      "        Whether to zero infinite losses and the associated gradients.\n",
      "        Default: ``False``\n",
      "        Infinite losses mainly occur when the inputs are too short\n",
      "        to be aligned to the targets.\n",
      "\n",
      "Shape:\n",
      "    - Log_probs: Tensor of size :math:`(T, N, C)` or :math:`(T, C)`,\n",
      "      where :math:`T = \\text{input length}`,\n",
      "      :math:`N = \\text{batch size}`, and\n",
      "      :math:`C = \\text{number of classes (including blank)}`.\n",
      "      The logarithmized probabilities of the outputs (e.g. obtained with\n",
      "      :func:`torch.nn.functional.log_softmax`).\n",
      "    - Targets: Tensor of size :math:`(N, S)` or\n",
      "      :math:`(\\operatorname{sum}(\\text{target\\_lengths}))`,\n",
      "      where :math:`N = \\text{batch size}` and\n",
      "      :math:`S = \\text{max target length, if shape is } (N, S)`.\n",
      "      It represents the target sequences. Each element in the target\n",
      "      sequence is a class index. And the target index cannot be blank (default=0).\n",
      "      In the :math:`(N, S)` form, targets are padded to the\n",
      "      length of the longest sequence, and stacked.\n",
      "      In the :math:`(\\operatorname{sum}(\\text{target\\_lengths}))` form,\n",
      "      the targets are assumed to be un-padded and\n",
      "      concatenated within 1 dimension.\n",
      "    - Input_lengths: Tuple or tensor of size :math:`(N)` or :math:`()`,\n",
      "      where :math:`N = \\text{batch size}`. It represents the lengths of the\n",
      "      inputs (must each be :math:`\\leq T`). And the lengths are specified\n",
      "      for each sequence to achieve masking under the assumption that sequences\n",
      "      are padded to equal lengths.\n",
      "    - Target_lengths: Tuple or tensor of size :math:`(N)` or :math:`()`,\n",
      "      where :math:`N = \\text{batch size}`. It represents lengths of the targets.\n",
      "      Lengths are specified for each sequence to achieve masking under the\n",
      "      assumption that sequences are padded to equal lengths. If target shape is\n",
      "      :math:`(N,S)`, target_lengths are effectively the stop index\n",
      "      :math:`s_n` for each target sequence, such that ``target_n = targets[n,0:s_n]`` for\n",
      "      each target in a batch. Lengths must each be :math:`\\leq S`\n",
      "      If the targets are given as a 1d tensor that is the concatenation of individual\n",
      "      targets, the target_lengths must add up to the total length of the tensor.\n",
      "    - Output: scalar if :attr:`reduction` is ``'mean'`` (default) or\n",
      "      ``'sum'``. If :attr:`reduction` is ``'none'``, then :math:`(N)` if input is batched or\n",
      "      :math:`()` if input is unbatched, where :math:`N = \\text{batch size}`.\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # Target are to be padded\n",
      "    >>> T = 50      # Input sequence length\n",
      "    >>> C = 20      # Number of classes (including blank)\n",
      "    >>> N = 16      # Batch size\n",
      "    >>> S = 30      # Target sequence length of longest target in batch (padding length)\n",
      "    >>> S_min = 10  # Minimum target length, for demonstration purposes\n",
      "    >>>\n",
      "    >>> # Initialize random batch of input vectors, for *size = (T,N,C)\n",
      "    >>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
      "    >>>\n",
      "    >>> # Initialize random batch of targets (0 = blank, 1:C = classes)\n",
      "    >>> target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
      "    >>>\n",
      "    >>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
      "    >>> target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
      "    >>> ctc_loss = nn.CTCLoss()\n",
      "    >>> loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
      "    >>> loss.backward()\n",
      "    >>>\n",
      "    >>>\n",
      "    >>> # Target are to be un-padded\n",
      "    >>> T = 50      # Input sequence length\n",
      "    >>> C = 20      # Number of classes (including blank)\n",
      "    >>> N = 16      # Batch size\n",
      "    >>>\n",
      "    >>> # Initialize random batch of input vectors, for *size = (T,N,C)\n",
      "    >>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
      "    >>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
      "    >>>\n",
      "    >>> # Initialize random batch of targets (0 = blank, 1:C = classes)\n",
      "    >>> target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n",
      "    >>> target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)\n",
      "    >>> ctc_loss = nn.CTCLoss()\n",
      "    >>> loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
      "    >>> loss.backward()\n",
      "    >>>\n",
      "    >>>\n",
      "    >>> # Target are to be un-padded and unbatched (effectively N=1)\n",
      "    >>> T = 50      # Input sequence length\n",
      "    >>> C = 20      # Number of classes (including blank)\n",
      "    >>>\n",
      "    >>> # Initialize random batch of input vectors, for *size = (T,C)\n",
      "    >>> # xdoctest: +SKIP(\"FIXME: error in doctest\")\n",
      "    >>> input = torch.randn(T, C).log_softmax(1).detach().requires_grad_()\n",
      "    >>> input_lengths = torch.tensor(T, dtype=torch.long)\n",
      "    >>>\n",
      "    >>> # Initialize random batch of targets (0 = blank, 1:C = classes)\n",
      "    >>> target_lengths = torch.randint(low=1, high=T, size=(), dtype=torch.long)\n",
      "    >>> target = torch.randint(low=1, high=C, size=(target_lengths,), dtype=torch.long)\n",
      "    >>> ctc_loss = nn.CTCLoss()\n",
      "    >>> loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
      "    >>> loss.backward()\n",
      "\n",
      "Reference:\n",
      "    A. Graves et al.: Connectionist Temporal Classification:\n",
      "    Labelling Unsegmented Sequence Data with Recurrent Neural Networks:\n",
      "    https://www.cs.toronto.edu/~graves/icml_2006.pdf\n",
      "\n",
      "Note:\n",
      "    In order to use CuDNN, the following must be satisfied: :attr:`targets` must be\n",
      "    in concatenated format, all :attr:`input_lengths` must be `T`.  :math:`blank=0`,\n",
      "    :attr:`target_lengths` :math:`\\leq 256`, the integer arguments must be of\n",
      "    dtype :attr:`torch.int32`.\n",
      "\n",
      "    The regular implementation uses the (more common in PyTorch) `torch.long` dtype.\n",
      "\n",
      "\n",
      "Note:\n",
      "    In some circumstances when using the CUDA backend with CuDNN, this operator\n",
      "    may select a nondeterministic algorithm to increase performance. If this is\n",
      "    undesirable, you can try to make the operation deterministic (potentially at\n",
      "    a performance cost) by setting ``torch.backends.cudnn.deterministic =\n",
      "    True``.\n",
      "    Please see the notes on :doc:`/notes/randomness` for background.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Downloads/ML/DLA/week04/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "torch.nn.CTCLoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.io_utils import ROOT_PATH\n",
    "import json\n",
    "\n",
    "data_dir = ROOT_PATH / \"data\" / \"datasets\" / \"librispeech\"\n",
    "index_path = data_dir / f\"{part}_index.json\"\n",
    "with index_path.open() as f:\n",
    "    index = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 (2.3%) records are longer then 20.0 seconds. Excluding them.\n",
      "59 (2.2%) records are longer then 300 characters. Excluding them.\n",
      "Filtered 75 (2.8%) records  from dataset\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_audio_length = 20.0\n",
    "max_text_length = 300\n",
    "\n",
    "initial_size = len(index)\n",
    "if max_audio_length is not None:\n",
    "    exceeds_audio_length = (\n",
    "        np.array([el[\"audio_len\"] for el in index]) >= max_audio_length\n",
    "    )\n",
    "    _total = exceeds_audio_length.sum()\n",
    "    print(\n",
    "        f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n",
    "        f\"{max_audio_length} seconds. Excluding them.\"\n",
    "    )\n",
    "else:\n",
    "    exceeds_audio_length = False\n",
    "\n",
    "initial_size = len(index)\n",
    "if max_text_length is not None:\n",
    "    exceeds_text_length = (\n",
    "        np.array(\n",
    "            [len(CTCTextEncoder.normalize_text(el[\"text\"])) for el in index]\n",
    "        )\n",
    "        >= max_text_length\n",
    "    )\n",
    "    _total = exceeds_text_length.sum()\n",
    "    print(\n",
    "        f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n",
    "        f\"{max_text_length} characters. Excluding them.\"\n",
    "    )\n",
    "else:\n",
    "    exceeds_text_length = False\n",
    "\n",
    "records_to_filter = exceeds_text_length | exceeds_audio_length\n",
    "\n",
    "if records_to_filter is not False and records_to_filter.any():\n",
    "    _total = records_to_filter.sum()\n",
    "    index = [el for el, exclude in zip(index, records_to_filter) if not exclude]\n",
    "    print(\n",
    "        f\"Filtered {_total} ({_total / initial_size:.1%}) records  from dataset\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "shuffle_index = True\n",
    "limit = 10\n",
    "\n",
    "if shuffle_index:\n",
    "    random.seed(42)\n",
    "    random.shuffle(index)\n",
    "\n",
    "if limit is not None:\n",
    "    index_ = index[:limit]\n",
    "# index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(target_sr, path):\n",
    "    audio_tensor, sr = torchaudio.load(path)\n",
    "    audio_tensor = audio_tensor[0:1, :]  # remove all channels but the first\n",
    "    target_sr = target_sr\n",
    "    if sr != target_sr:\n",
    "        audio_tensor = torchaudio.functional.resample(audio_tensor, sr, target_sr)\n",
    "    return audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(instance_transforms, instance_data):\n",
    "    if instance_transforms is not None:\n",
    "        for transform_name in instance_transforms.keys():\n",
    "            if transform_name == \"get_spectrogram\":\n",
    "                continue  # skip special key\n",
    "            instance_data[transform_name] = instance_transforms[\n",
    "                transform_name\n",
    "            ](instance_data[transform_name])\n",
    "    return instance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getitem_(ind):\n",
    "    target_sr=16000\n",
    "    data_dict = index_[ind]\n",
    "    audio_path = data_dict[\"path\"]\n",
    "    audio = load_audio(target_sr=target_sr, path=audio_path)\n",
    "    text = data_dict[\"text\"]\n",
    "    text_encoded = text_encoder.encode(text)\n",
    "\n",
    "    spectrogram = instance_transforms[\"get_spectrogram\"](audio)\n",
    "\n",
    "    instance_data = {\n",
    "        \"audio\": audio,\n",
    "        \"spectrogram\": spectrogram,\n",
    "        \"text\": text,\n",
    "        \"text_encoded\": text_encoded,\n",
    "        \"audio_path\": audio_path,\n",
    "    }\n",
    "\n",
    "\n",
    "    instance_data_ = preprocess_data(instance_transforms, instance_data)\n",
    "    return instance_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 55920]), torch.Size([1, 120960]), torch.Size([1, 47360]), torch.Size([1, 242400]), torch.Size([1, 131600]), torch.Size([1, 158400]), torch.Size([1, 192640]), torch.Size([1, 108160]), torch.Size([1, 125520]), torch.Size([1, 75840])]\n",
      "[torch.Size([1, 128, 280]), torch.Size([1, 128, 605]), torch.Size([1, 128, 237]), torch.Size([1, 128, 1213]), torch.Size([1, 128, 659]), torch.Size([1, 128, 793]), torch.Size([1, 128, 964]), torch.Size([1, 128, 541]), torch.Size([1, 128, 628]), torch.Size([1, 128, 380])]\n",
      "[torch.Size([1, 38]), torch.Size([1, 97]), torch.Size([1, 53]), torch.Size([1, 222]), torch.Size([1, 100]), torch.Size([1, 169]), torch.Size([1, 160]), torch.Size([1, 65]), torch.Size([1, 131]), torch.Size([1, 55])]\n"
     ]
    }
   ],
   "source": [
    "audio_shapes, spectrogram_shapes, text_encoded_shapes = [], [], []\n",
    "for ind in range(len(index_)):\n",
    "    audio_shapes.append(getitem_(ind)['audio'].shape)\n",
    "    spectrogram_shapes.append(getitem_(ind)['spectrogram'].shape)\n",
    "    text_encoded_shapes.append(getitem_(ind)['text_encoded'].shape)\n",
    "\n",
    "print(audio_shapes, spectrogram_shapes, text_encoded_shapes, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_items = []\n",
    "for ind in range(len(index_)):\n",
    "    dataset_items.append(getitem_(ind))\n",
    "# dataset_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# For BaseModel\n",
    "audios = [item['audio'].squeeze() for item in dataset_items]\n",
    "spectrograms = [item['spectrogram'].squeeze().transpose(0,1) for item in dataset_items]\n",
    "texts = [item['text'] for item in dataset_items]\n",
    "text_encoded = [item['text_encoded'].squeeze() for item in dataset_items]\n",
    "audio_paths = [item['audio_path'] for item in dataset_items]\n",
    "\n",
    "# Pad audios, spectrograms and text_encoded sequences\n",
    "padded_audios = pad_sequence(audios, batch_first=True, padding_value=0)\n",
    "padded_spectrograms = pad_sequence(spectrograms, batch_first=True, padding_value=0).transpose(1,2)\n",
    "padded_text_encoded = pad_sequence(text_encoded, batch_first=True, padding_value=0)\n",
    "\n",
    "# Create the result batch dictionary\n",
    "result_batch = {\n",
    "    'audio': padded_audios,\n",
    "    'spectrogram': padded_spectrograms,\n",
    "    'text_encoded': padded_text_encoded,\n",
    "    'text': texts,\n",
    "    'audio_path': audio_paths\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (asr_project)",
   "language": "python",
   "name": "asr_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
