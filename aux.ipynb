{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from src.datasets.data_utils import get_dataloaders\n",
    "from src.trainer import Trainer\n",
    "from src.utils.init_utils import set_random_seed, setup_saving_and_logging\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ostywise/Downloads/ML/DLA/week04/.venv/lib/python3.12/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
      "  >>> augment = Gain(..., output_type='dict')\n",
      "  >>> augmented_samples = augment(samples).samples\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.text_encoder import CTCTextEncoder\n",
    "from src.datasets import LibrispeechDataset\n",
    "from src.transforms.wav_augs import Gain\n",
    "from src.datasets.collate import collate_fn\n",
    "import torch\n",
    "import torchaudio \n",
    "\n",
    "part=\"dev-clean\"\n",
    "instance_transforms= {\n",
    "    'get_spectrogram': torchaudio.transforms.MelSpectrogram(sample_rate=16000),\n",
    "    'audio': Gain()\n",
    "}\n",
    "\n",
    "text_encoder = CTCTextEncoder()\n",
    "dataset = LibrispeechDataset(\n",
    "    text_encoder=text_encoder,\n",
    "    part= part,\n",
    "    max_audio_length= 20.0,\n",
    "    max_text_length= 200,\n",
    "    limit= 10,\n",
    "    instance_transforms= instance_transforms\n",
    ")\n",
    "# dataloaders, batch_transforms = get_dataloaders(config, text_encoder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    dataset=dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': tensor([[-0.0007, -0.0077, -0.0030,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0004, -0.0002,  0.0012,  ..., -0.0003, -0.0004, -0.0004]]),\n",
       " 'spectrogram': tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [1.9926e-04, 3.0717e-03, 1.4854e-03,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [1.0729e-03, 1.6539e-02, 7.9980e-03,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [7.9302e-05, 3.6578e-05, 5.7361e-05,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [1.0455e-04, 6.7070e-05, 3.9266e-05,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [1.8559e-04, 2.9940e-05, 6.8682e-05,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [7.7752e-04, 9.0164e-03, 4.0364e-03,  ..., 4.7646e-02,\n",
       "           5.4324e-02, 9.7332e-03],\n",
       "          [4.1864e-03, 4.8547e-02, 2.1733e-02,  ..., 2.5654e-01,\n",
       "           2.9250e-01, 5.2406e-02],\n",
       "          ...,\n",
       "          [1.2331e-03, 2.1994e-03, 4.5385e-03,  ..., 2.2218e-05,\n",
       "           7.5958e-05, 4.9155e-06],\n",
       "          [1.7305e-03, 2.0639e-03, 3.0732e-03,  ..., 1.1167e-05,\n",
       "           2.5350e-05, 1.9196e-06],\n",
       "          [6.4001e-04, 2.6581e-03, 2.9864e-03,  ..., 3.7077e-05,\n",
       "           4.4088e-05, 2.4069e-06]]]),\n",
       " 'text_encoded': tensor([[ 4., 15., 19., 20., 27., 20.,  8., 15., 21., 27., 18.,  9., 19.,  5.,\n",
       "          27.,  1.,  7.,  1.,  9., 14., 19., 20., 27.,  8.,  9., 13., 27., 23.,\n",
       "           8., 15., 27.,  7.,  9., 22.,  5., 19., 27., 20.,  8.,  5.,  5., 27.,\n",
       "           8.,  9., 19., 27.,  2., 18.,  5.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "           0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 8.,  5., 27., 19.,  1., 23., 27., 20.,  8.,  1., 20., 27.,  8.,  9.,\n",
       "          19., 27., 19., 17., 21.,  9., 18.,  5., 19., 27.,  6., 15., 18., 27.,\n",
       "          19., 15., 27., 20.,  8.,  5., 25., 27.,  3.,  1., 12., 12., 27., 20.,\n",
       "           8., 15., 19.,  5., 27., 23.,  8., 15., 27.,  6., 15., 12., 12., 15.,\n",
       "          23., 27., 20.,  8.,  1., 20., 27., 20., 18.,  1.,  4.,  5., 27., 23.,\n",
       "           5., 18.,  5., 27.,  1.,  2., 15., 21., 20., 27., 20., 15., 27., 18.,\n",
       "           9.,  6., 12.,  5., 27., 19.,  1., 14.,  3.,  8., 15., 27., 16.,  1.,\n",
       "          14., 26.,  1., 27.,  2., 21., 20., 27.,  8.,  5., 27., 15., 18.,  4.,\n",
       "           5., 18.,  5.,  4., 27., 20.,  8.,  5., 13., 27., 20., 15., 27.,  4.,\n",
       "           5., 19.,  9., 19., 20., 27.,  1., 14.,  4., 27., 23.,  1., 19., 27.,\n",
       "           1., 20., 27., 15., 14.,  3.,  5., 27., 15.,  2.,  5., 25.,  5.,  4.,\n",
       "          27., 19., 15., 27., 20.,  8.,  5., 27.,  7.,  9., 18.,  4., 12.,  5.,\n",
       "          27.,  5., 19.,  3.,  1., 16.,  5.,  4.]]),\n",
       " 'text': ['dost thou rise against him who gives thee his bread',\n",
       "  'he saw that his squires for so they call those who follow that trade were about to rifle sancho panza but he ordered them to desist and was at once obeyed so the girdle escaped'],\n",
       " 'audio_path': ['/Users/ostywise/Downloads/ML/DLA/asr_project/data/datasets/librispeech/dev-clean/3576/138058/3576-138058-0008.flac',\n",
       "  '/Users/ostywise/Downloads/ML/DLA/asr_project/data/datasets/librispeech/dev-clean/3576/138058/3576-138058-0016.flac'],\n",
       " 'spectrogram_length': tensor([309, 956], dtype=torch.int32),\n",
       " 'text_encoded_length': tensor([ 51, 176], dtype=torch.int32)}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 51, 176], dtype=torch.int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['text_encoded_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CTCLoss\n",
    "criterion = CTCLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.io_utils import ROOT_PATH\n",
    "import json\n",
    "\n",
    "data_dir = ROOT_PATH / \"data\" / \"datasets\" / \"librispeech\"\n",
    "index_path = data_dir / f\"{part}_index.json\"\n",
    "with index_path.open() as f:\n",
    "    index = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 (2.3%) records are longer then 20.0 seconds. Excluding them.\n",
      "59 (2.2%) records are longer then 300 characters. Excluding them.\n",
      "Filtered 75 (2.8%) records  from dataset\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_audio_length = 20.0\n",
    "max_text_length = 300\n",
    "\n",
    "initial_size = len(index)\n",
    "if max_audio_length is not None:\n",
    "    exceeds_audio_length = (\n",
    "        np.array([el[\"audio_len\"] for el in index]) >= max_audio_length\n",
    "    )\n",
    "    _total = exceeds_audio_length.sum()\n",
    "    print(\n",
    "        f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n",
    "        f\"{max_audio_length} seconds. Excluding them.\"\n",
    "    )\n",
    "else:\n",
    "    exceeds_audio_length = False\n",
    "\n",
    "initial_size = len(index)\n",
    "if max_text_length is not None:\n",
    "    exceeds_text_length = (\n",
    "        np.array(\n",
    "            [len(CTCTextEncoder.normalize_text(el[\"text\"])) for el in index]\n",
    "        )\n",
    "        >= max_text_length\n",
    "    )\n",
    "    _total = exceeds_text_length.sum()\n",
    "    print(\n",
    "        f\"{_total} ({_total / initial_size:.1%}) records are longer then \"\n",
    "        f\"{max_text_length} characters. Excluding them.\"\n",
    "    )\n",
    "else:\n",
    "    exceeds_text_length = False\n",
    "\n",
    "records_to_filter = exceeds_text_length | exceeds_audio_length\n",
    "\n",
    "if records_to_filter is not False and records_to_filter.any():\n",
    "    _total = records_to_filter.sum()\n",
    "    index = [el for el, exclude in zip(index, records_to_filter) if not exclude]\n",
    "    print(\n",
    "        f\"Filtered {_total} ({_total / initial_size:.1%}) records  from dataset\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "shuffle_index = True\n",
    "limit = 10\n",
    "\n",
    "if shuffle_index:\n",
    "    random.seed(42)\n",
    "    random.shuffle(index)\n",
    "\n",
    "if limit is not None:\n",
    "    index_ = index[:limit]\n",
    "# index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(target_sr, path):\n",
    "    audio_tensor, sr = torchaudio.load(path)\n",
    "    audio_tensor = audio_tensor[0:1, :]  # remove all channels but the first\n",
    "    target_sr = target_sr\n",
    "    if sr != target_sr:\n",
    "        audio_tensor = torchaudio.functional.resample(audio_tensor, sr, target_sr)\n",
    "    return audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(instance_transforms, instance_data):\n",
    "    if instance_transforms is not None:\n",
    "        for transform_name in instance_transforms.keys():\n",
    "            if transform_name == \"get_spectrogram\":\n",
    "                continue  # skip special key\n",
    "            instance_data[transform_name] = instance_transforms[\n",
    "                transform_name\n",
    "            ](instance_data[transform_name])\n",
    "    return instance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getitem_(ind):\n",
    "    target_sr=16000\n",
    "    data_dict = index_[ind]\n",
    "    audio_path = data_dict[\"path\"]\n",
    "    audio = load_audio(target_sr=target_sr, path=audio_path)\n",
    "    text = data_dict[\"text\"]\n",
    "    text_encoded = text_encoder.encode(text)\n",
    "\n",
    "    spectrogram = instance_transforms[\"get_spectrogram\"](audio)\n",
    "\n",
    "    instance_data = {\n",
    "        \"audio\": audio,\n",
    "        \"spectrogram\": spectrogram,\n",
    "        \"text\": text,\n",
    "        \"text_encoded\": text_encoded,\n",
    "        \"audio_path\": audio_path,\n",
    "    }\n",
    "\n",
    "\n",
    "    instance_data_ = preprocess_data(instance_transforms, instance_data)\n",
    "    return instance_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 55920]), torch.Size([1, 120960]), torch.Size([1, 47360]), torch.Size([1, 242400]), torch.Size([1, 131600]), torch.Size([1, 158400]), torch.Size([1, 192640]), torch.Size([1, 108160]), torch.Size([1, 125520]), torch.Size([1, 75840])]\n",
      "[torch.Size([1, 128, 280]), torch.Size([1, 128, 605]), torch.Size([1, 128, 237]), torch.Size([1, 128, 1213]), torch.Size([1, 128, 659]), torch.Size([1, 128, 793]), torch.Size([1, 128, 964]), torch.Size([1, 128, 541]), torch.Size([1, 128, 628]), torch.Size([1, 128, 380])]\n",
      "[torch.Size([1, 38]), torch.Size([1, 97]), torch.Size([1, 53]), torch.Size([1, 222]), torch.Size([1, 100]), torch.Size([1, 169]), torch.Size([1, 160]), torch.Size([1, 65]), torch.Size([1, 131]), torch.Size([1, 55])]\n"
     ]
    }
   ],
   "source": [
    "audio_shapes, spectrogram_shapes, text_encoded_shapes = [], [], []\n",
    "for ind in range(len(index_)):\n",
    "    audio_shapes.append(getitem_(ind)['audio'].shape)\n",
    "    spectrogram_shapes.append(getitem_(ind)['spectrogram'].shape)\n",
    "    text_encoded_shapes.append(getitem_(ind)['text_encoded'].shape)\n",
    "\n",
    "print(audio_shapes, spectrogram_shapes, text_encoded_shapes, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_items = []\n",
    "for ind in range(len(index_)):\n",
    "    dataset_items.append(getitem_(ind))\n",
    "# dataset_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# For BaseModel\n",
    "audios = [item['audio'].squeeze() for item in dataset_items]\n",
    "spectrograms = [item['spectrogram'].squeeze().transpose(0,1) for item in dataset_items]\n",
    "texts = [item['text'] for item in dataset_items]\n",
    "text_encoded = [item['text_encoded'].squeeze() for item in dataset_items]\n",
    "audio_paths = [item['audio_path'] for item in dataset_items]\n",
    "\n",
    "# Pad audios, spectrograms and text_encoded sequences\n",
    "padded_audios = pad_sequence(audios, batch_first=True, padding_value=0)\n",
    "padded_spectrograms = pad_sequence(spectrograms, batch_first=True, padding_value=0).transpose(1,2)\n",
    "padded_text_encoded = pad_sequence(text_encoded, batch_first=True, padding_value=0)\n",
    "\n",
    "# Create the result batch dictionary\n",
    "result_batch = {\n",
    "    'audio': padded_audios,\n",
    "    'spectrogram': padded_spectrograms,\n",
    "    'text_encoded': padded_text_encoded,\n",
    "    'text': texts,\n",
    "    'audio_path': audio_paths\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.conformer import Conformer, ConformerEncoder, ConformerBlock, LSTMDecoder, \\\n",
    "                                Conv2dSubsampling, FeedForwardBlock, ConvBlock, RelativeMultiHeadAttention, PositionalEncoder\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_input = 956\n",
    "d_model = 144\n",
    "encoder_num_layers = 16\n",
    "num_heads = 4\n",
    "kernel_size = 31\n",
    "dropout = 0.1\n",
    "feed_forward_residual_factor = 0.5\n",
    "feed_forward_expansion_factor = 4\n",
    "d_decoder = 320\n",
    "decoder_num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_subsample = Conv2dSubsampling(d_model=d_model)\n",
    "linear_proj = nn.Linear(d_model * ((((d_input - 3)//2 + 1) - 3)//2 + 1), d_model)  # project subsamples to d_model\n",
    "dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "# define global positional encoder to limit model parameters\n",
    "positional_encoder = PositionalEncoder(d_model)\n",
    "layers = nn.ModuleList([ConformerBlock(\n",
    "        d_model=d_model,\n",
    "        kernel_size=kernel_size,\n",
    "        feed_forward_residual_factor=feed_forward_residual_factor,\n",
    "        feed_forward_expansion_factor=feed_forward_expansion_factor,\n",
    "        num_heads=num_heads,\n",
    "        positional_encoder=positional_encoder,\n",
    "        dropout=dropout\n",
    "    ) for _ in range(encoder_num_layers)])\n",
    "\n",
    "def forward(x, mask=None):\n",
    "    x = conv_subsample(x)\n",
    "    if mask is not None:\n",
    "        mask = mask[:, :-2:2, :-2:2]  # account for subsampling\n",
    "        mask = mask[:, :-2:2, :-2:2]  # account for subsampling\n",
    "        assert mask.shape[1] == x.shape[1], f'{mask.shape} {x.shape}'\n",
    "        \n",
    "    x = linear_proj(x)\n",
    "    x = dropout_layer(x)\n",
    "\n",
    "    for layer in layers:\n",
    "        x = layer(x, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 956])\n"
     ]
    }
   ],
   "source": [
    "x = batch['spectrogram']\n",
    "print(x.shape)\n",
    "B, D, T = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 31, 34272])\n"
     ]
    }
   ],
   "source": [
    "x = conv_subsample(x)\n",
    "print(x.shape)\n",
    "\n",
    "# (B, (((D - 3)//2 + 1) - 3)//2 + 1, d_model * ((((d_input - 3)//2 + 1) - 3)//2 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 31, 144])\n"
     ]
    }
   ],
   "source": [
    "x = linear_proj(x)\n",
    "x = dropout_layer(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    x = layer(x, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 144])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # (B, (((D - 3)//2 + 1) - 3)//2 + 1, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=d_model, hidden_size=d_decoder, num_layers=decoder_num_layers, batch_first=True)\n",
    "linear = nn.Linear(d_decoder, 28)\n",
    "\n",
    "def forward(self, x):\n",
    "    x, _ = self.lstm(x)\n",
    "    logits = self.linear(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 320])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_lstm, _ = lstm(x)\n",
    "x_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = linear(x_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 28])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([309, 956], dtype=torch.int32)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['spectrogram_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_probs': tensor([[[-3.3323, -3.3108, -3.3361,  ..., -3.3956, -3.2959, -3.2685],\n",
       "          [-3.3289, -3.3358, -3.2979,  ..., -3.4059, -3.3126, -3.2541],\n",
       "          [-3.3644, -3.4266, -3.2446,  ..., -3.3554, -3.3281, -3.1901],\n",
       "          ...,\n",
       "          [-3.3651, -3.1910, -3.2827,  ..., -3.3874, -3.3250, -3.2776],\n",
       "          [-3.4006, -3.2088, -3.2813,  ..., -3.4104, -3.2806, -3.2939],\n",
       "          [-3.4238, -3.2461, -3.2897,  ..., -3.3975, -3.2951, -3.2552]],\n",
       " \n",
       "         [[-3.4573, -3.3119, -3.3816,  ..., -3.3381, -3.3896, -3.2692],\n",
       "          [-3.4425, -3.3378, -3.4109,  ..., -3.3623, -3.3256, -3.3057],\n",
       "          [-3.4132, -3.2908, -3.4506,  ..., -3.3858, -3.3482, -3.3471],\n",
       "          ...,\n",
       "          [-3.3580, -3.2561, -3.3572,  ..., -3.3753, -3.3795, -3.2994],\n",
       "          [-3.3785, -3.2847, -3.3927,  ..., -3.4138, -3.3738, -3.3354],\n",
       "          [-3.3365, -3.2762, -3.3231,  ..., -3.3392, -3.3141, -3.2813]]],\n",
       "        grad_fn=<LogSoftmaxBackward0>),\n",
       " 'log_probs_length': tensor([309, 956], dtype=torch.int32)}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "log_probs_length = batch['spectrogram_length']\n",
    "{\"log_probs\": log_probs, \"log_probs_length\": log_probs_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 28])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (asr_project)",
   "language": "python",
   "name": "asr_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
